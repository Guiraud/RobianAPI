"""
Router pour les d√©bats de l'Assembl√©e nationale
"""

import asyncio
from typing import List, Optional
from datetime import datetime, date

from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, or_, and_
from sqlalchemy.orm import joinedload

from ..models import get_db_session, Debate, AudioFile, DebateType, DebateStatus
from ..schemas import (
    DebateResponse, 
    DebateListResponse, 
    DebateSearchFilters,
    DebateCreate,
    DebateUpdate
)
from ..services import (
    cache_service, 
    websocket_manager, 
    WebSocketMessage, 
    MessageType
)
import structlog

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/api/debates", tags=["debates"])


@router.get("/", response_model=DebateListResponse)
async def list_debates(
    q: Optional[str] = Query(None, description="Recherche textuelle"),
    type: Optional[DebateType] = Query(None, description="Type de d√©bat"),
    status: Optional[DebateStatus] = Query(None, description="Statut du d√©bat"),
    commission: Optional[str] = Query(None, description="Commission"),
    date_start: Optional[date] = Query(None, description="Date de d√©but"),
    date_end: Optional[date] = Query(None, description="Date de fin"),
    has_audio: Optional[bool] = Query(None, description="Avec audio disponible"),
    page: int = Query(1, ge=1, description="Num√©ro de page"),
    per_page: int = Query(20, ge=1, le=100, description="√âl√©ments par page"),
    sort_by: str = Query("date", description="Champ de tri"),
    sort_order: str = Query("desc", regex="^(asc|desc)$", description="Ordre de tri"),
    db: AsyncSession = Depends(get_db_session)
):
    """
    Liste des d√©bats avec recherche et filtres avanc√©s
    Utilise le cache Redis pour les performances
    """
    
    # Cl√© de cache bas√©e sur tous les param√®tres
    cache_key = f"list_{q}_{type}_{status}_{commission}_{date_start}_{date_end}_{has_audio}_{page}_{per_page}_{sort_by}_{sort_order}"
    
    # Tentative de r√©cup√©ration depuis le cache
    cached_result = await cache_service.get("debates", cache_key)
    if cached_result:
        logger.debug("üì¶ D√©bats depuis cache Redis", page=page, per_page=per_page)
        return cached_result
    
    try:
        # Construction de la requ√™te de base
        query = select(Debate).options(joinedload(Debate.audio_files))
        count_query = select(func.count(Debate.id))
        
        # Filtres
        filters = []
        
        if q:
            # Recherche textuelle dans titre et description
            search_filter = or_(
                Debate.title.ilike(f"%{q}%"),
                Debate.description.ilike(f"%{q}%"),
                Debate.speakers.any(q),  # Recherche dans le tableau speakers
                Debate.tags.any(q)       # Recherche dans le tableau tags
            )
            filters.append(search_filter)
        
        if type:
            filters.append(Debate.type == type)
        
        if status:
            filters.append(Debate.status == status)
        
        if commission:
            filters.append(Debate.commission.ilike(f"%{commission}%"))
        
        if date_start:
            filters.append(Debate.date >= date_start)
        
        if date_end:
            filters.append(Debate.date <= date_end)
        
        if has_audio is not None:
            if has_audio:
                # D√©bats avec au moins un fichier audio pr√™t
                filters.append(
                    Debate.audio_files.any(AudioFile.extraction_status == "completed")
                )
            else:
                # D√©bats sans fichier audio ou en cours d'extraction
                filters.append(
                    or_(
                        ~Debate.audio_files.any(),
                        ~Debate.audio_files.any(AudioFile.extraction_status == "completed")
                    )
                )
        
        # Appliquer tous les filtres
        if filters:
            query = query.where(and_(*filters))
            count_query = count_query.where(and_(*filters))
        
        # Tri
        sort_column = getattr(Debate, sort_by, Debate.date)
        if sort_order == "desc":
            query = query.order_by(sort_column.desc())
        else:
            query = query.order_by(sort_column.asc())
        
        # Pagination
        offset = (page - 1) * per_page
        query = query.offset(offset).limit(per_page)
        
        # Ex√©cution des requ√™tes
        result = await db.execute(query)
        debates = result.scalars().unique().all()
        
        count_result = await db.execute(count_query)
        total = count_result.scalar()
        
        # Conversion en sch√©mas de r√©ponse
        debate_responses = [DebateResponse.model_validate(debate) for debate in debates]
        
        # Calcul de la pagination
        has_next = (page * per_page) < total
        has_prev = page > 1
        
        response = DebateListResponse(
            debates=debate_responses,
            total=total,
            page=page,
            per_page=per_page,
            has_next=has_next,
            has_prev=has_prev
        )
        
        # Mise en cache pour 5 minutes
        await cache_service.set("debates", cache_key, response.model_dump(), ttl=300)
        
        logger.info("üì∫ D√©bats r√©cup√©r√©s depuis base de donn√©es", 
                   count=len(debates), page=page, total=total)
        
        return response
        
    except Exception as e:
        logger.error("‚ùå Erreur r√©cup√©ration d√©bats", error=str(e))
        raise HTTPException(status_code=500, detail="Erreur lors de la r√©cup√©ration des d√©bats")


@router.get("/{debate_id}", response_model=DebateResponse)
async def get_debate(
    debate_id: str,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db_session)
):
    """
    R√©cup√©rer les d√©tails d'un d√©bat sp√©cifique
    Incr√©mente le compteur de vues de mani√®re asynchrone
    """
    
    # Cache check
    cache_key = f"detail_{debate_id}"
    cached_debate = await cache_service.get("debates", cache_key)
    if cached_debate:
        # Incr√©menter le compteur de vues en arri√®re-plan
        background_tasks.add_task(increment_view_count, debate_id, db)
        return DebateResponse(**cached_debate)
    
    try:
        # Requ√™te avec jointure pour r√©cup√©rer les fichiers audio
        query = select(Debate).options(joinedload(Debate.audio_files)).where(Debate.id == debate_id)
        result = await db.execute(query)
        debate = result.scalars().first()
        
        if not debate:
            raise HTTPException(status_code=404, detail=f"D√©bat {debate_id} non trouv√©")
        
        # Conversion en sch√©ma de r√©ponse
        debate_response = DebateResponse.model_validate(debate)
        
        # Cache pour 1 heure
        await cache_service.set("debates", cache_key, debate_response.model_dump(), ttl=3600)
        
        # Incr√©menter le compteur de vues en arri√®re-plan
        background_tasks.add_task(increment_view_count, debate_id, db)
        
        # Notifier via WebSocket
        await websocket_manager.broadcast_to_channel(
            f"debate:{debate_id}",
            WebSocketMessage(
                type=MessageType.SYSTEM_STATUS,
                channel=f"debate:{debate_id}",
                data={"action": "viewed", "debate_id": debate_id}
            )
        )
        
        logger.info("üì∫ D√©tail d√©bat r√©cup√©r√©", debate_id=debate_id)
        return debate_response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("‚ùå Erreur r√©cup√©ration d√©bat", debate_id=debate_id, error=str(e))
        raise HTTPException(status_code=500, detail="Erreur lors de la r√©cup√©ration du d√©bat")


@router.post("/", response_model=DebateResponse)
async def create_debate(
    debate_data: DebateCreate,
    db: AsyncSession = Depends(get_db_session)
):
    """
    Cr√©er un nouveau d√©bat
    (G√©n√©ralement utilis√© par les scripts de scraping)
    """
    
    try:
        # V√©rifier si un d√©bat avec la m√™me URL source existe d√©j√†
        existing_query = select(Debate).where(Debate.source_url == debate_data.source_url)
        existing_result = await db.execute(existing_query)
        existing_debate = existing_result.scalars().first()
        
        if existing_debate:
            raise HTTPException(status_code=409, detail="Un d√©bat avec cette URL source existe d√©j√†")
        
        # Cr√©er le nouveau d√©bat
        debate = Debate(**debate_data.model_dump())
        db.add(debate)
        await db.commit()
        await db.refresh(debate)
        
        # Invalider le cache des listes
        await cache_service.clear_namespace("debates")
        
        # Notifier via WebSocket
        await websocket_manager.broadcast_to_channel(
            "debates",
            WebSocketMessage(
                type=MessageType.DEBATE_STARTED if debate.status == DebateStatus.EN_COURS else MessageType.SYSTEM_STATUS,
                channel="debates",
                data={"action": "created", "debate": debate.to_dict()}
            )
        )
        
        logger.info("‚úÖ Nouveau d√©bat cr√©√©", debate_id=debate.id, title=debate.title)
        
        return DebateResponse.model_validate(debate)
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error("‚ùå Erreur cr√©ation d√©bat", error=str(e))
        raise HTTPException(status_code=500, detail="Erreur lors de la cr√©ation du d√©bat")


@router.put("/{debate_id}", response_model=DebateResponse)
async def update_debate(
    debate_id: str,
    debate_data: DebateUpdate,
    db: AsyncSession = Depends(get_db_session)
):
    """
    Mettre √† jour un d√©bat existant
    """
    
    try:
        # R√©cup√©rer le d√©bat existant
        query = select(Debate).where(Debate.id == debate_id)
        result = await db.execute(query)
        debate = result.scalars().first()
        
        if not debate:
            raise HTTPException(status_code=404, detail=f"D√©bat {debate_id} non trouv√©")
        
        # Mettre √† jour les champs fournis
        update_data = debate_data.model_dump(exclude_unset=True)
        for field, value in update_data.items():
            setattr(debate, field, value)
        
        await db.commit()
        await db.refresh(debate)
        
        # Invalider le cache
        await cache_service.delete("debates", f"detail_{debate_id}")
        await cache_service.clear_namespace("debates")
        
        logger.info("üìù D√©bat mis √† jour", debate_id=debate_id)
        
        return DebateResponse.model_validate(debate)
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error("‚ùå Erreur mise √† jour d√©bat", debate_id=debate_id, error=str(e))
        raise HTTPException(status_code=500, detail="Erreur lors de la mise √† jour du d√©bat")


@router.delete("/{debate_id}")
async def delete_debate(
    debate_id: str,
    db: AsyncSession = Depends(get_db_session)
):
    """
    Supprimer un d√©bat
    (Utilisation administrative uniquement)
    """
    
    try:
        # R√©cup√©rer le d√©bat
        query = select(Debate).where(Debate.id == debate_id)
        result = await db.execute(query)
        debate = result.scalars().first()
        
        if not debate:
            raise HTTPException(status_code=404, detail=f"D√©bat {debate_id} non trouv√©")
        
        # Supprimer (cascade supprimera aussi les audio_files)
        await db.delete(debate)
        await db.commit()
        
        # Invalider le cache
        await cache_service.delete("debates", f"detail_{debate_id}")
        await cache_service.clear_namespace("debates")
        
        logger.info("üóëÔ∏è D√©bat supprim√©", debate_id=debate_id)
        
        return {"message": f"D√©bat {debate_id} supprim√© avec succ√®s"}
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error("‚ùå Erreur suppression d√©bat", debate_id=debate_id, error=str(e))
        raise HTTPException(status_code=500, detail="Erreur lors de la suppression du d√©bat")


async def increment_view_count(debate_id: str, db: AsyncSession):
    """
    Incr√©menter le compteur de vues de mani√®re asynchrone
    """
    try:
        query = select(Debate).where(Debate.id == debate_id)
        result = await db.execute(query)
        debate = result.scalars().first()
        
        if debate:
            debate.view_count += 1
            await db.commit()
            
            # Invalider le cache pour forcer la mise √† jour
            await cache_service.delete("debates", f"detail_{debate_id}")
            
            logger.debug("üëÄ Vue incr√©ment√©e", debate_id=debate_id, new_count=debate.view_count)
        
    except Exception as e:
        await db.rollback()
        logger.error("‚ùå Erreur incr√©mentation vue", debate_id=debate_id, error=str(e))


# Routes de compatibilit√© avec l'ancienne API
@router.get("/legacy/debats", include_in_schema=False)
async def legacy_list_debates(
    date_debut: Optional[str] = None,
    date_fin: Optional[str] = None,
    type_debat: Optional[str] = None,
    commission: Optional[str] = None,
    limit: int = 50,
    db: AsyncSession = Depends(get_db_session)
):
    """
    Route de compatibilit√© pour l'ancienne API
    Redirige vers la nouvelle API moderne
    """
    
    # Convertir les param√®tres de l'ancienne API vers la nouvelle
    filters = {}
    if type_debat:
        filters["type"] = type_debat
    if commission:
        filters["commission"] = commission
    if date_debut:
        try:
            filters["date_start"] = datetime.fromisoformat(date_debut).date()
        except ValueError:
            pass
    if date_fin:
        try:
            filters["date_end"] = datetime.fromisoformat(date_fin).date()
        except ValueError:
            pass
    
    # Appeler la nouvelle API
    return await list_debates(
        per_page=min(limit, 100),
        db=db,
        **filters
    )
